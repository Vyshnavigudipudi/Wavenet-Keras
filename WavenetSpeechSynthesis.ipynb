{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1FLfrSS5YyTGt9dk5WaRdChjfScPVSxev",
      "authorship_tag": "ABX9TyNIIBaqtvU3bpid0A4JadbC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vyshnavigudipudi/Wavenet-Keras/blob/main/WavenetSpeechSynthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip uninstall tensorflow-addons -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rpme6kiwk6s",
        "outputId": "37d9305f-b1b7-4178-a235-1c98f6e321de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.17.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? \u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/uninstall.py\", line 106, in run\n",
            "    uninstall_pathset = req.uninstall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 722, in uninstall\n",
            "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 364, in remove\n",
            "    if auto_confirm or self._allowed_to_proceed(verbose):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 404, in _allowed_to_proceed\n",
            "    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 235, in ask\n",
            "    response = input(message)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 396, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 365, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 323, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Found existing installation: tensorflow-addons 0.23.0\n",
            "Uninstalling tensorflow-addons-0.23.0:\n",
            "  Successfully uninstalled tensorflow-addons-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWUR_ezhxE__",
        "outputId": "029ffe48-a9ce-4686-8387-bb32f57101e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.11.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.64.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
            "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.23.1 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons==0.17.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj9qgvAOxSHo",
        "outputId": "57465a06-5cc6-4227-ce30-8f75fed94a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons==0.17.1\n",
            "  Downloading tensorflow_addons-0.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.17.1) (24.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.17.1) (2.13.3)\n",
            "Downloading tensorflow_addons-0.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-warn-conflicts -q tensorflow-addons"
      ],
      "metadata": {
        "id": "q5RPVpkTxmvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJt8ASB4x5Bm",
        "outputId": "f493dddd-ba12-40cc-bbff-fb9260efbb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from PyWavelets) (1.26.4)\n",
            "Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/4.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/4.5 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n",
        "                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN,\n",
        "                                     RepeatVector, Conv1D, MaxPooling1D, Concatenate, GlobalAveragePooling1D, UpSampling1D)\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras import losses, models, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\n",
        "from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\n",
        "from sklearn.model_selection import KFold, GroupKFold\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from contextlib import contextmanager\n",
        "from joblib import Parallel, delayed\n",
        "from IPython.display import display\n",
        "from sklearn import preprocessing\n",
        "import tensorflow_addons as tfa\n",
        "import scipy.stats as stats\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import itertools\n",
        "import warnings\n",
        "import time\n",
        "import pywt\n",
        "import os\n",
        "import gc\n",
        "\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "EhvvmNW6xsQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=120\n",
        "NNBATCHSIZE=20\n",
        "BATCHSIZE = 4000\n",
        "SEED = 321\n",
        "SELECT = True\n",
        "SPLITS = 5\n",
        "LR = 0.001\n",
        "fe_config = [\n",
        "    (True, 4000),\n",
        "]"
      ],
      "metadata": {
        "id": "jCWlbKqDx-Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def init_logger():\n",
        "    handler = StreamHandler()\n",
        "    handler.setLevel(INFO)\n",
        "    handler.setFormatter(Formatter(LOGFORMAT))\n",
        "    fh_handler = FileHandler('{}.log'.format(MODELNAME))\n",
        "    fh_handler.setFormatter(Formatter(LOGFORMAT))\n",
        "    logger.setLevel(INFO)\n",
        "    logger.addHandler(handler)\n",
        "    logger.addHandler(fh_handler)\n"
      ],
      "metadata": {
        "id": "o1PIhxQ0yFwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@contextmanager\n",
        "def timer(name : Text):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
        "\n",
        "COMPETITION = 'ION-Switching'\n",
        "logger = getLogger(COMPETITION)\n",
        "LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
        "MODELNAME = 'WaveNet'\n"
      ],
      "metadata": {
        "id": "i2_AbeE0yMhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def seed_everything(seed : int) -> NoReturn :\n",
        "\n",
        "    rn.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "seed_everything(SEED)\n"
      ],
      "metadata": {
        "id": "aRzkUq1fyVDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os # imports the os module\n",
        "from typing import Tuple # imports the Tuple type\n",
        "import pandas as pd # imports the pandas module and aliases it as pd\n",
        "import numpy as np # imports the numpy module and aliases it as np\n",
        "\n",
        "def read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "\n",
        "    train = pd.read_csv('/content/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
        "    test  = pd.read_csv('/content/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
        "    sub  = pd.read_csv('/content/sample_submission.csv', dtype={'time': np.float32})\n",
        "\n",
        "    return train, test, sub"
      ],
      "metadata": {
        "id": "wb7KA2lTyeKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def batching(df : pd.DataFrame,\n",
        "             batch_size : int) -> pd.DataFrame :\n",
        "\n",
        "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
        "    df['group'] = df['group'].astype(np.uint16)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Tv-siCvtyn3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def reduce_mem_usage(df: pd.DataFrame,\n",
        "                     verbose: bool = True) -> pd.DataFrame:\n",
        "\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "\n",
        "            if str(col_type)[:3] == 'int':\n",
        "\n",
        "                if (c_min > np.iinfo(np.int32).min\n",
        "                      and c_max < np.iinfo(np.int32).max):\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif (c_min > np.iinfo(np.int64).min\n",
        "                      and c_max < np.iinfo(np.int64).max):\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if (c_min > np.finfo(np.float16).min\n",
        "                        and c_max < np.finfo(np.float16).max):\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif (c_min > np.finfo(np.float32).min\n",
        "                      and c_max < np.finfo(np.float32).max):\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    reduction = (start_mem - end_mem) / start_mem\n",
        "\n",
        "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
        "    if verbose:\n",
        "        print(msg)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "VGWjYPV7ytlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def lag_with_pct_change(df : pd.DataFrame,\n",
        "                        shift_sizes : Optional[List]=[1, 2],\n",
        "                        add_pct_change : Optional[bool]=False,\n",
        "                        add_pct_change_lag : Optional[bool]=False) -> pd.DataFrame:\n",
        "\n",
        "    for shift_size in shift_sizes:\n",
        "        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(0)\n",
        "        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(0)\n",
        "\n",
        "    if add_pct_change:\n",
        "        df['pct_change'] = df['signal'].pct_change()\n",
        "        if add_pct_change_lag:\n",
        "            for shift_size in shift_sizes:\n",
        "                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(0)\n",
        "                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(0)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "sFLcx15wy1oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_feat_enginnering(df : pd.DataFrame,\n",
        "                         create_all_data_feats : bool,\n",
        "                         batch_size : int) -> pd.DataFrame:\n",
        "\n",
        "    df = batching(df, batch_size=batch_size)\n",
        "    if create_all_data_feats:\n",
        "        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False)\n",
        "    df['signal_2'] = df['signal'] ** 2\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "HnIDlPDqy_1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_selection(df : pd.DataFrame,\n",
        "                      df_test : pd.DataFrame) -> Tuple[pd.DataFrame , pd.DataFrame, List]:\n",
        "    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n",
        "    for col in use_cols:\n",
        "        col_mean = pd.concat([df[col], df_test[col]], axis=0).mean()\n",
        "        df[col] = df[col].fillna(col_mean)\n",
        "        df_test[col] = df_test[col].fillna(col_mean)\n",
        "\n",
        "    gc.collect()\n",
        "    return df, df_test, use_cols\n"
      ],
      "metadata": {
        "id": "ACHopQVtzFEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n",
        "\n",
        "    X = np.vstack((X, np.flip(X, axis=1)))\n",
        "    y = np.vstack((y, np.flip(y, axis=1)))\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "kiYVT1MwzK-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_cv_model_by_batch(train : pd.DataFrame,\n",
        "                          test : pd.DataFrame,\n",
        "                          splits : int,\n",
        "                          batch_col : Text,\n",
        "                          feats : List,\n",
        "                          sample_submission: pd.DataFrame,\n",
        "                          nn_epochs : int,\n",
        "                          nn_batch_size : int) -> NoReturn:\n",
        "    seed_everything(SEED)\n",
        "    K.clear_session()\n",
        "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\n",
        "    oof_ = np.zeros((len(train), 11))\n",
        "    preds_ = np.zeros((len(test), 11))\n",
        "    target = ['open_channels']\n",
        "    group = train['group']\n",
        "    kf = GroupKFold(n_splits=5)\n",
        "    splits = [x for x in kf.split(train, train[target], group)]\n",
        "\n",
        "    new_splits = []\n",
        "    for sp in splits:\n",
        "        new_split = []\n",
        "        new_split.append(np.unique(group[sp[0]]))\n",
        "        new_split.append(np.unique(group[sp[1]]))\n",
        "        new_split.append(sp[1])\n",
        "        new_splits.append(new_split)\n",
        "\n",
        "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
        "\n",
        "    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
        "    target_cols = ['target_'+str(i) for i in range(11)]\n",
        "    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
        "    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
        "    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
        "\n",
        "    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n",
        "        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n",
        "        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n",
        "\n",
        "        if n_fold < 2:\n",
        "            train_x, train_y = augment(train_x, train_y)\n",
        "\n",
        "        gc.collect()\n",
        "        shape_ = (None, train_x.shape[2])\n",
        "        model = Classifier(shape_)\n",
        "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
        "        cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, show_epoch_progress=False,show_overall_progress=True)\n",
        "        model.fit(train_x,train_y,\n",
        "                  epochs=nn_epochs,\n",
        "                  callbacks=[cb_prg, cb_lr_schedule, MacroF1(model, valid_x,valid_y)],\n",
        "                  batch_size=nn_batch_size,verbose=0,\n",
        "                  validation_data=(valid_x,valid_y))\n",
        "        preds_f = model.predict(valid_x)\n",
        "        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n",
        "        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
        "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
        "        oof_[val_orig_idx,:] += preds_f\n",
        "        te_preds = model.predict(test)\n",
        "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])\n",
        "        preds_ += te_preds / SPLITS\n",
        "    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis=1), average = 'macro')\n",
        "    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
        "    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n",
        "    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\n",
        "    display(sample_submission.head())\n",
        "    np.save('oof.npy', oof_)\n",
        "    np.save('preds.npy', preds_)\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "CC12FwP8zZmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch < 40:\n",
        "        lr = LR\n",
        "    elif epoch < 50:\n",
        "        lr = LR / 3\n",
        "    elif epoch < 60:\n",
        "        lr = LR / 6\n",
        "    elif epoch < 75:\n",
        "        lr = LR / 9\n",
        "    elif epoch < 85:\n",
        "        lr = LR / 12\n",
        "    elif epoch < 100:\n",
        "        lr = LR / 15\n",
        "    else:\n",
        "        lr = LR / 50\n",
        "    return lr"
      ],
      "metadata": {
        "id": "v7yWqyuqzfTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mish(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Mish, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs * K.tanh(K.softplus(inputs))\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(Mish, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "def mish(x):\n",
        "\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n",
        "\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from tensorflow.keras.layers import Activation\n",
        "get_custom_objects().update({'mish': Activation(mish)})"
      ],
      "metadata": {
        "id": "bjNlt6tGzlj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "\n",
        "class Attention(Layer):\n",
        "    \"\"\"Multi-headed attention layer.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size,\n",
        "                 num_heads = 8,\n",
        "                 attention_dropout=.1,\n",
        "                 trainable=True,\n",
        "                 name='Attention'):\n",
        "\n",
        "        if hidden_size % num_heads != 0:\n",
        "            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.trainable = trainable\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n",
        "        super(Attention, self).__init__(name=name)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"Split x into different heads, and transpose the resulting value.\n",
        "        The tensor is transposed to insure the inner dimensions hold the correct\n",
        "        values during the matrix multiplication.\n",
        "        Args:\n",
        "          x: A tensor with shape [batch_size, length, hidden_size]\n",
        "        Returns:\n",
        "          A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"split_heads\"):\n",
        "            batch_size = tf.shape(x)[0]\n",
        "            length = tf.shape(x)[1]\n",
        "\n",
        "            # Calculate depth of last dimension after it has been split.\n",
        "            depth = (self.hidden_size // self.num_heads)\n",
        "\n",
        "            # Split the last dimension\n",
        "            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n",
        "\n",
        "            # Transpose the result\n",
        "            return tf.transpose(x, [0, 2, 1, 3])\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"Combine tensor that has been split.\n",
        "        Args:\n",
        "          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n",
        "        Returns:\n",
        "          A tensor with shape [batch_size, length, hidden_size]\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"combine_heads\"):\n",
        "            batch_size = tf.shape(x)[0]\n",
        "            length = tf.shape(x)[2]\n",
        "            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n",
        "            return tf.reshape(x, [batch_size, length, self.hidden_size])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply attention mechanism to inputs.\n",
        "        Args:\n",
        "          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n",
        "        Returns:\n",
        "          Attention layer output with shape [batch_size, length_x, hidden_size]\n",
        "        \"\"\"\n",
        "        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n",
        "        q = self.dense(inputs)\n",
        "        k = self.dense(inputs)\n",
        "        v = self.dense(inputs)\n",
        "\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "\n",
        "        # Scale q to prevent the dot product between q and k from growing too large.\n",
        "        depth = (self.hidden_size // self.num_heads)\n",
        "        q *= depth ** -0.5\n",
        "\n",
        "        logits = tf.matmul(q, k, transpose_b=True)\n",
        "        # logits += self.bias\n",
        "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
        "\n",
        "        if self.trainable:\n",
        "            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n",
        "\n",
        "        attention_output = tf.matmul(weights, v)\n",
        "        attention_output = self.combine_heads(attention_output)\n",
        "        attention_output = self.dense(attention_output)\n",
        "        return attention_output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape(input_shape)"
      ],
      "metadata": {
        "id": "8_1iL7UEzsLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_focal_loss(gamma=2.0, alpha=0.25):\n",
        "    \"\"\"\n",
        "    Implementation of Focal Loss from the paper in multiclass classification\n",
        "    Formula:\n",
        "        loss = -alpha*((1-p)^gamma)*log(p)\n",
        "    Parameters:\n",
        "        alpha -- the same as wighting factor in balanced cross entropy\n",
        "        gamma -- focusing parameter for modulating factor (1-p)\n",
        "    Default value:\n",
        "        gamma -- 2.0 as mentioned in the paper\n",
        "        alpha -- 0.25 as mentioned in the paper\n",
        "    \"\"\"\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        # Define epsilon so that the backpropagation will not result in NaN\n",
        "        # for 0 divisor case\n",
        "        epsilon = K.epsilon()\n",
        "        # Add the epsilon to prediction value\n",
        "        #y_pred = y_pred + epsilon\n",
        "        # Clip the prediction value\n",
        "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
        "        # Calculate cross entropy\n",
        "        cross_entropy = -y_true*K.log(y_pred)\n",
        "        # Calculate weight that consists of  modulating factor and weighting factor\n",
        "        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n",
        "        # Calculate focal loss\n",
        "        loss = weight * cross_entropy\n",
        "        # Sum the losses in mini_batch\n",
        "        loss = K.sum(loss, axis=1)\n",
        "        return loss\n",
        "\n",
        "    return focal_loss"
      ],
      "metadata": {
        "id": "dcNTOpAgz057"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n",
        "\n",
        "    def build_residual_block(l_input):\n",
        "        resid_input = l_input\n",
        "        for dilation_rate in [2**i for i in range(stacked_layer)]:\n",
        "            l_sigmoid_conv1d = Conv1D(\n",
        "              num_filters, kernel_size, dilation_rate=dilation_rate,\n",
        "              padding='same', activation='sigmoid')(l_input)\n",
        "            l_tanh_conv1d = Conv1D(\n",
        "             num_filters, kernel_size, dilation_rate=dilation_rate,\n",
        "             padding='same', activation='mish')(l_input)\n",
        "            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n",
        "            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n",
        "            resid_input = Add()([resid_input ,l_input])\n",
        "        return resid_input\n",
        "    return build_residual_block\n",
        "def Classifier(shape_):\n",
        "    num_filters_ = 16\n",
        "    kernel_size_ = 3\n",
        "    stacked_layers_ = [12, 8, 4, 1]\n",
        "    l_input = Input(shape=(shape_))\n",
        "    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n",
        "    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
        "    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
        "    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
        "    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
        "    l_output = Dense(11, activation='softmax')(x)\n",
        "    model = models.Model(inputs=[l_input], outputs=[l_output])\n",
        "    opt = Adam(lr=LR)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "cyHwbAau03xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Classifierx(shape_):\n",
        "\n",
        "    inp = Input(shape=(shape_))\n",
        "    x = Bidirectional(GRU(256, return_sequences=True))(inp)\n",
        "    x = Attention(512)(x)\n",
        "    x = TimeDistributed(Dense(256, activation='mish'))(x)\n",
        "    x = TimeDistributed(Dense(128, activation='mish'))(x)\n",
        "    out = TimeDistributed(Dense(11, activation='softmax', name='out'))(x)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "\n",
        "    opt = Adam(lr=LR)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "vz3t0ya409ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback # import Callback object\n",
        "\n",
        "class MacroF1(Callback):\n",
        "    def __init__(self, model, inputs, targets):\n",
        "        self.model = model\n",
        "        self.inputs = inputs\n",
        "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
        "        score = f1_score(self.targets, pred, average=\"macro\")\n",
        "        print(f' F1Macro: {score:.5f}')"
      ],
      "metadata": {
        "id": "gNj2JvwQ1JHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(train, test):\n",
        "\n",
        "    train_input_mean = train.signal.mean()\n",
        "    train_input_sigma = train.signal.std()\n",
        "    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n",
        "    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "g70N0DlY1Pt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from typing import List, NoReturn # import List object from typing module\n",
        "import logging # import the logging module\n",
        "from contextlib import contextmanager # import contextmanager from contextlib\n",
        "import time # import the time module\n",
        "import os # import the os module\n",
        "import pandas as pd # import pandas module\n",
        "\n",
        "# Instead of importing from a file, directly define the normalize function here\n",
        "def normalize(train, test):\n",
        "\n",
        "    train_input_mean = train.signal.mean()\n",
        "    train_input_sigma = train.signal.std()\n",
        "    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n",
        "    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n",
        "\n",
        "    return train, test\n",
        "\n",
        "@contextmanager\n",
        "def timer(title): # define the timer function using contextmanager\n",
        "    t_start = time.time() # call time.time() to get the current time\n",
        "    yield\n",
        "    t_end = time.time() # call time.time() to get the current time\n",
        "    print(\"{} - done in {:.0f}s\".format(title, t_end - t_start)) # print the title and the time taken\n",
        "\n",
        "def init_logger(): # define the init_logger function\n",
        "    logging.basicConfig(level=logging.INFO) # set the logging level to INFO\n",
        "\n",
        "def read_data(base):\n",
        "    '''\n",
        "    Reads the training and testing data from the specified base directory.\n",
        "\n",
        "    Args:\n",
        "        base (str): The base directory containing the data files.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the training data, testing data, and sample submission.\n",
        "    '''\n",
        "    train = pd.read_csv(f'/content/train_clean_kalman.csv')\n",
        "    test = pd.read_csv(f'/content/test_clean_kalman.csv')\n",
        "    sample_submission = pd.read_csv(f'/content/sample_submission.csv')\n",
        "    return train, test, sample_submission\n",
        "\n",
        "def run_everything(fe_config : List) -> NoReturn:\n",
        "    not_feats_cols = ['time']\n",
        "    target_col = ['open_channels']\n",
        "    init_logger() # call the init_logger function\n",
        "    with timer(f'Reading Data'): # use the timer function with the title 'Reading Data'\n",
        "        logger = logging.getLogger(__name__) # define logger\n",
        "        logger.info('Reading Data Started ...')\n",
        "        base = os.path.abspath('/kaggle/input/liverpool-ion-switching/')\n",
        "        train, test, sample_submission = read_data(base)\n",
        "        train, test = normalize(train, test)\n",
        "        logger.info('Reading and Normalizing Data Completed ...')\n",
        "    with timer(f'Creating Features'): # use the timer function with the title 'Creating Features'\n",
        "        logger.info('Feature Enginnering Started ...')\n",
        "        for config in fe_config:\n",
        "            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n",
        "            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n",
        "        train, test, feats = feature_selection(train, test)\n",
        "        logger.info('Feature Enginnering Completed ...')\n",
        "\n",
        "    with timer(f'Running Wavenet model'): # use the timer function with the title 'Running Wavenet model'\n",
        "        logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n",
        "        run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n",
        "        logger.info(f'Training completed ...')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "u7etTuDpaPdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from typing import List, NoReturn # import List object from typing module\n",
        "import logging # import the logging module\n",
        "from contextlib import contextmanager # import contextmanager from contextlib\n",
        "import time # import the time module\n",
        "import os # import the os module\n",
        "import pandas as pd # import pandas module\n",
        "\n",
        "# Instead of importing from a file, directly define the normalize function here\n",
        "def normalize(train, test):\n",
        "\n",
        "    train_input_mean = train.signal.mean()\n",
        "    train_input_sigma = train.signal.std()\n",
        "    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n",
        "    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n",
        "\n",
        "    return train, test\n",
        "\n",
        "@contextmanager\n",
        "def timer(title): # define the timer function using contextmanager\n",
        "    t_start = time.time() # call time.time() to get the current time\n",
        "    yield\n",
        "    t_end = time.time() # call time.time() to get the current time\n",
        "    print(\"{} - done in {:.0f}s\".format(title, t_end - t_start)) # print the title and the time taken\n",
        "\n",
        "def init_logger(): # define the init_logger function\n",
        "    logging.basicConfig(level=logging.INFO) # set the logging level to INFO\n",
        "\n",
        "def read_data(base):\n",
        "    '''\n",
        "    Reads the training and testing data from the specified base directory.\n",
        "\n",
        "    Args:\n",
        "        base (str): The base directory containing the data files.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the training data, testing data, and sample submission.\n",
        "    '''\n",
        "    train = pd.read_csv(f'/content/train_clean_kalman.csv')\n",
        "    test = pd.read_csv(f'/content/test_clean_kalman.csv')\n",
        "    sample_submission = pd.read_csv(f'/content/sample_submission.csv')\n",
        "    return train, test, sample_submission\n",
        "\n",
        "# define a placeholder for the missing function\n",
        "def run_feat_enginnering(df, create_all_data_feats=False, batch_size=100000):\n",
        "    return df\n",
        "\n",
        "def run_everything(fe_config : List) -> NoReturn:\n",
        "    not_feats_cols = ['time']\n",
        "    target_col = ['open_channels']\n",
        "    init_logger() # call the init_logger function\n",
        "    with timer(f'Reading Data'): # use the timer function with the title 'Reading Data'\n",
        "        logger = logging.getLogger(__name__) # define logger\n",
        "        logger.info('Reading Data Started ...')\n",
        "        base = os.path.abspath('/kaggle/input/liverpool-ion-switching/')\n",
        "        train, test, sample_submission = read_data(base)\n",
        "        train, test = normalize(train, test)\n",
        "        logger.info('Reading and Normalizing Data Completed ...')\n",
        "    with timer(f'Creating Features'): # use the timer function with the title 'Creating Features'\n",
        "        logger.info('Feature Enginnering Started ...')\n",
        "        for config in fe_config:\n",
        "            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n",
        "            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n",
        "        # Comment out the following line as feature_selection is not defined\n",
        "        # train, test, feats = feature_selection(train, test)\n",
        "        logger.info('Feature Enginnering Completed ...')\n",
        "\n",
        "    # Comment out the following block as the referenced functions are not defined\n",
        "    # with timer(f'Running Wavenet model'): # use the timer function with the title 'Running Wavenet model'\n",
        "    #     logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n",
        "    #     run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n",
        "    #     logger.info(f'Training completed ...')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "G7sw7sbqajH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fe_config = [\n",
        "    [False, 100000],\n",
        "] # define fe_config as a list of lists\n",
        "run_everything(fe_config) # call the function with the defined variable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bnaPpLp1fIM",
        "outputId": "bfc6aae2-328d-494e-b620-e86786470fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Data - done in 4s\n",
            "Creating Features - done in 0s\n"
          ]
        }
      ]
    }
  ]
}